% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MOBenchmark.R
\name{MOBenchmark}
\alias{MOBenchmark}
\title{Multi-Omics Benchmark}
\description{
Use this \link[R6:R6Class]{R6} class to execute a multi-omics
benchmark using several \link[=SurvLPS]{survival learners}.

The use-case here is that we have several datasets representing
different omic profiles for a particular patient cohort.
For example we have mRNA expression, CNA, methylation and clinical data for
a breast cancer patient cohort.
We can choose to generate all possible combinations of omic datasets
(referred to as tasks) or simply benchmark the given tasks (individual omics
or other datasets).
We train and tune several survival models on the same proportion of patients
(\strong{train cohort}) across all tasks.
Performance is assessed using various survival measures on bootstrap
resamplings of a \strong{separate test cohort}.

See example below.
}
\examples{
library(mlr3proba)

}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{tasks}}{Survival datasets to benchmark}

\item{\code{gen_task_powerset}}{Generate task powerset?}

\item{\code{part}}{List partitioning the tasks to train/tuning and test sets}

\item{\code{lrn_ids}}{Internal survival learner ids to benchmark}

\item{\code{nthreads_rsf}}{Number of cores for random survival forests}

\item{\code{nthreads_xgb}}{Number of cores for xgboost survival learners}

\item{\code{tune_rsmp}}{Tuning resampling}

\item{\code{tune_measure_id}}{Survival measure for tuning}

\item{\code{tune_nevals}}{Number of evaluations during Bayesian Optimization
tuning}

\item{\code{test_measure_ids}}{Survival measures for testing}

\item{\code{test_nrsmps}}{Number of bootstrap resamplings of the test set}

\item{\code{test_workers}}{Number of cores for bootstrap parallelization}

\item{\code{keep_models}}{Keep the trained/tuned models?}

\item{\code{quiet}}{Show elapsed times for training and testing?}

\item{\code{result}}{Tibble result with the benchmark results}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-MultiOmicsBenchmark-new}{\code{MOBenchmark$new()}}
\item \href{#method-MultiOmicsBenchmark-run}{\code{MOBenchmark$run()}}
\item \href{#method-MultiOmicsBenchmark-tune_learner}{\code{MOBenchmark$tune_learner()}}
\item \href{#method-MultiOmicsBenchmark-drop_tasks}{\code{MOBenchmark$drop_tasks()}}
\item \href{#method-MultiOmicsBenchmark-drop_models}{\code{MOBenchmark$drop_models()}}
\item \href{#method-MultiOmicsBenchmark-clone}{\code{MOBenchmark$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MultiOmicsBenchmark-new"></a>}}
\if{latex}{\out{\hypertarget{method-MultiOmicsBenchmark-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$new(
  tasks,
  gen_task_powerset = TRUE,
  part,
  lrn_ids = NULL,
  nthreads_rsf = unname(parallelly::availableCores()),
  nthreads_xgb = 2,
  tune_rsmp = rsmp("repeated_cv", repeats = 5, folds = 5),
  tune_measure_id = "uno_c",
  tune_nevals = 100,
  test_measure_ids = c("uno_c", "rcll"),
  test_nrsmps = 1000,
  test_workers = 1,
  keep_models = FALSE,
  quiet = TRUE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{tasks}}{List of \link[mlr3proba:TaskSurv]{TaskSurv}.}

\item{\code{gen_task_powerset}}{Whether to generate the
\link[=task_powerset]{task powerset} from the given \code{tasks}.
If TRUE (default), then the \code{tasks} should be different omic datasets,
with the same target variables (\code{time}, \code{status}), i.e. corresponding to
the same patient cohort. If FALSE, we use \code{tasks} as is.}

\item{\code{part}}{List with a disjoint (common) partition of the tasks to
\code{train} and \code{test} indexes.
The training/tuning is performed on the \code{train} set and the
bootstrap testing on the \code{test} set for each task.}

\item{\code{lrn_ids}}{Internal learner ids to use for benchmarking.
Default: NULL, i.e. use all available learners, see
\link[=SurvLPS]{supported_lrn_ids()}.}

\item{\code{nthreads_rsf}}{Number of cores to use in random survival forest
learners (implicit parallelization).
Default: use all available cores.}

\item{\code{nthreads_xgb}}{Number of cores to use in xgboost survival learners
(implicit parallelization).
Default: use 2 cores.
Never use only 1 core since during parallel bootstrap resampling xgboost
learners behave erratically in terms of CPU usage.}

\item{\code{tune_rsmp}}{\link[mlr3:Resampling]{Resampling} to use for tuning the
learners on the \code{part$train} set.
Default is repeated-CV (5 folds, 5 times).}

\item{\code{tune_measure_id}}{Internal measure id to use for tuning the learners.
Default is Uno's C-index.
See \code{\link[=bench_msrs]{bench_msrs()}} for available measures.}

\item{\code{tune_nevals}}{Number of evaluations (hyperparameter configurations)
to try during Bayesian Optimization tuning before termination.
Default: 100.}

\item{\code{test_measure_ids}}{Internal measure ids to use for assessing the
performance of the learners in the test set (\code{part$test}).
See \code{\link[=bench_msrs]{bench_msrs()}} for available measures.}

\item{\code{test_nrsmps}}{Number of bootstrap resamplings of the test set.
Must be >= 1. Default: 1000.}

\item{\code{test_workers}}{Number of workers for parallelization of the bootstrap
resampling on the test set. Default: 1.
This value is overridden when benchmarking xgboost learners and set to 1
to avoid strange overuse of CPU.}

\item{\code{keep_models}}{Whether to keep the trained models after tuning.
Default: FALSE.}

\item{\code{quiet}}{Whether to report elapsed timings for training and testing.
Default: TRUE (\strong{don't} report).}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MultiOmicsBenchmark-run"></a>}}
\if{latex}{\out{\hypertarget{method-MultiOmicsBenchmark-run}{}}}
\subsection{Method \code{run()}}{
Execute benchmark
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$run()}\if{html}{\out{</div>}}
}

\subsection{Details}{
Benchmark steps:
\itemize{
\item Generate task powerset (if \code{task_powerset = TRUE})
\item Generate grid of learners and tasks
\item For each (learner, task) combo:
\itemize{
\item Tune each learner in the train set
\item Test the tuned learner's performance by taking multiple bootstrap
resamplings of the test set in the corresponding task and applying
several test measures
}
}
}

\subsection{Returns}{
a \link{tibble} with columns:
\itemize{
\item \code{task_id} => which task
\item \code{lrn_id} => which learner
\item \code{model} => the trained learner after tuning (if \code{keep_models = TRUE})
\item \code{boot_res} => a \link{BootstrapResult} object (with the bootstrap results)
}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MultiOmicsBenchmark-tune_learner"></a>}}
\if{latex}{\out{\hypertarget{method-MultiOmicsBenchmark-tune_learner}{}}}
\subsection{Method \code{tune_learner()}}{
Tune \code{learner} on the \code{given} task.
The tuning is done on the train set of the task (parameter \code{part}),
using a specific performance measure (parameter \code{tuning_measure_id}) and
resampling (parameter \code{tune_rsmp}).

The tuning is performed by constructing an \link[mlr3tuning:AutoTuner]{AutoTuner}
object and the optimization strategy is by default \strong{Bayesian Optimization}.
Optimization stops after a specific number of evaluations, see parameter
\code{tune_nevals}.
The hyperparameter configuration with the \strong{best} average resampled
performance is chosen to train a final model which is returned by this
function.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$tune_learner(task, learner, search_space)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{task}}{a \link[mlr3proba:TaskSurv]{TaskSurv}}

\item{\code{learner}}{a \link[mlr3proba:LearnerSurv]{LearnerSurv}}

\item{\code{search_space}}{a \link[paradox:ParamSet]{ParamSet} indicating the
learner's hyperparameters that need to be tuned.
If \code{NULL}, we just \code{train} the learner on the \code{task}.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MultiOmicsBenchmark-drop_tasks"></a>}}
\if{latex}{\out{\hypertarget{method-MultiOmicsBenchmark-drop_tasks}{}}}
\subsection{Method \code{drop_tasks()}}{
remove tasks from the class object to reduce size
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$drop_tasks()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MultiOmicsBenchmark-drop_models"></a>}}
\if{latex}{\out{\hypertarget{method-MultiOmicsBenchmark-drop_models}{}}}
\subsection{Method \code{drop_models()}}{
remove models from result tibble to reduce size
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$drop_models()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MultiOmicsBenchmark-clone"></a>}}
\if{latex}{\out{\hypertarget{method-MultiOmicsBenchmark-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
