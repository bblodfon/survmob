% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MOBenchmark.R
\name{MOBenchmark}
\alias{MOBenchmark}
\title{Multi-Omics Benchmark}
\description{
Use this \link[R6:R6Class]{R6} class to execute a multi-omics
benchmark using several \link[=SurvLPS]{survival learners}.

The use-case here is that we have several datasets representing
different omic profiles for a particular patient cohort.
For example we have mRNA expression, CNA, methylation and clinical data for
a breast cancer patient cohort.
We can choose to generate all possible combinations of omic datasets
(referred to as tasks) or simply benchmark the given tasks (individual omics
or other datasets).
We train and tune several survival models on the same proportion of patients
(\strong{train cohort}) across all tasks.
Performance is assessed using various survival measures on bootstrap
resamplings of a \strong{separate test cohort}.

The most important thing to keep in mind is that this class was made to
\strong{benchmark datasets that share the same observations} (maybe not
conceptually in case of separate given tasks and \code{gen_task_powerset = FALSE}
but most definitely in number).
See example below.
}
\examples{
library(mlr3verse)
library(mlr3mbo)
library(mlr3proba)
library(future)

# Logging
lgr::get_logger('bbotk')$set_threshold('warn')
lgr::get_logger('mlr3')$set_threshold('warn')

# task lung
task = tsk('lung')
pre = po('encode', method = 'treatment') \%>>\%
      po('imputelearner', lrn('regr.rpart'))
task = pre$train(task)[[1]]
task$filter(1:100) # subsample for faster execution

# task veteran
task2 = as_task_surv(x = survival::veteran, id = 'veteran',
  time = 'time', event = 'status')
task2 = pre$train(task2)[[1L]]
task2$filter(1:100) # subsample for faster execution

# partition to train and test sets
part = partition(task, ratio = 0.8)

mob = MOBenchmark$new(
  tasks = list(task, task2), part = part,
  gen_task_powerset = FALSE,
  lrn_ids = c('coxph', 'coxnet', 'aorsf'), use_callr = FALSE,
  tune_nevals = 2, test_nrsmps = 10, test_workers = 1,
  tune_rsmp = rsmp('holdout', ratio = 0.8),
  quiet = FALSE, keep_models = TRUE
)

# execute benchmark
mob$run()

# result tibble
mob$result

# reshape benchmarking results
df = reshape_mob_res(mob$result)
df

# Run Bayesian LME model
plan(multisession, workers = 2) # 2 measures tested by default
res = fit_blme_model_cmp(df, n_chains = 2, n_iters = 2000)

}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{tasks}}{Survival datasets to benchmark}

\item{\code{gen_task_powerset}}{Generate task powerset?}

\item{\code{part}}{List partitioning the tasks to train/tuning and test sets}

\item{\code{lrn_ids}}{Internal survival learner ids to benchmark}

\item{\code{use_callr}}{Whether to encapsulate specific learners with \link{callr}}

\item{\code{nthreads_rsf}}{Number of cores for random survival forests}

\item{\code{nthreads_xgb}}{Number of cores for xgboost survival learners}

\item{\code{tune_rsmp}}{Tuning resampling}

\item{\code{tune_measure_id}}{Survival measure for tuning}

\item{\code{tune_nevals}}{Number of evaluations during Bayesian Optimization
tuning}

\item{\code{test_measure_ids}}{Survival measures for testing}

\item{\code{test_nrsmps}}{Number of bootstrap resamplings of the test set}

\item{\code{test_workers}}{Number of cores for bootstrap parallelization}

\item{\code{keep_models}}{Keep the trained/tuned models?}

\item{\code{quiet}}{Show elapsed times for training/testing and other
informative messages?}

\item{\code{result}}{Tibble result with the benchmark results}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-MOBenchmark-new}{\code{MOBenchmark$new()}}
\item \href{#method-MOBenchmark-run}{\code{MOBenchmark$run()}}
\item \href{#method-MOBenchmark-tune_learner}{\code{MOBenchmark$tune_learner()}}
\item \href{#method-MOBenchmark-drop_tasks}{\code{MOBenchmark$drop_tasks()}}
\item \href{#method-MOBenchmark-drop_models}{\code{MOBenchmark$drop_models()}}
\item \href{#method-MOBenchmark-clone}{\code{MOBenchmark$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MOBenchmark-new"></a>}}
\if{latex}{\out{\hypertarget{method-MOBenchmark-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$new(
  tasks,
  gen_task_powerset = TRUE,
  part,
  lrn_ids = NULL,
  use_callr = TRUE,
  nthreads_rsf = unname(parallelly::availableCores()),
  nthreads_xgb = 2,
  tune_rsmp = rsmp("repeated_cv", repeats = 5, folds = 5),
  tune_measure_id = "uno_c",
  tune_nevals = 100,
  test_measure_ids = c("uno_c", "rcll"),
  test_nrsmps = 1000,
  test_workers = 1,
  keep_models = FALSE,
  quiet = FALSE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{tasks}}{List of \link[mlr3proba:TaskSurv]{TaskSurv}.}

\item{\code{gen_task_powerset}}{Whether to generate the
\link[=task_powerset]{task powerset} from the given \code{tasks}.
If TRUE (default), then the \code{tasks} should be different omic datasets,
with the same target variables (\code{time}, \code{status}), i.e. corresponding to
the same patient cohort.
If FALSE, we use \code{tasks} as is, but you need to make sure they have the
same number of observations (rows), otherwise the common partition
to train and test sets (\code{part}) will not be tasks.}

\item{\code{part}}{List with a disjoint (common) partition of the tasks to
\code{train} and \code{test} indexes.
The training/tuning is performed on the \code{train} set and the
bootstrap testing on the \code{test} set for each task.
Use the \link[mlr3:partition]{partition} function to perform stratified
train/test splitting on the \code{status} indicator variable.}

\item{\code{lrn_ids}}{Internal learner ids to use for benchmarking.
Default is NULL, i.e. use all available learners.
See \link[=SurvLPS]{supported_lrn_ids} for which ids can be used.}

\item{\code{use_callr}}{Whether to encapsulate specific learners with \link{callr}.
See \link{SurvLPS} initialization. Default is TRUE.}

\item{\code{nthreads_rsf}}{Number of cores to use in random survival forest
learners (implicit parallelization).
Default: use all available cores.}

\item{\code{nthreads_xgb}}{Number of cores to use in xgboost survival learners
(implicit parallelization).
Default: use 2 cores.
Never use only 1 core since during bootstrap resampling xgboost
learners behave erratically in terms of CPU usage.}

\item{\code{tune_rsmp}}{\link[mlr3:Resampling]{Resampling} to use for tuning the
learners on the \code{part$train} set.
Default is repeated-CV (5 folds, 5 times).}

\item{\code{tune_measure_id}}{Internal measure id to use for tuning the learners.
Default is Uno's C-index.
See \code{\link[=bench_msrs]{bench_msrs()}} for available measures.}

\item{\code{tune_nevals}}{Number of evaluations (hyperparameter configurations)
to try during Bayesian Optimization tuning before termination.
Default: 100.}

\item{\code{test_measure_ids}}{Internal measure ids to use for assessing the
performance of the learners in the test set (\code{part$test}).
See \code{\link[=bench_msrs]{bench_msrs()}} for available measures.}

\item{\code{test_nrsmps}}{Number of bootstrap resamplings of the test set.
Must be >= 1. Default: 1000.}

\item{\code{test_workers}}{Number of workers for parallelization of the bootstrap
resampling on the test set.
This should be configured high enough based on available CPU cores.
Default is 1.
The value is temporarily overridden when benchmarking xgboost learners
and set to 1 to avoid strange overuse of CPU.}

\item{\code{keep_models}}{Whether to keep the trained models after tuning.
Default: FALSE.}

\item{\code{quiet}}{Whether to report elapsed timings for tuning/testing and
other informative messages.
Default: FALSE (\strong{Report timings and show messages}).}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MOBenchmark-run"></a>}}
\if{latex}{\out{\hypertarget{method-MOBenchmark-run}{}}}
\subsection{Method \code{run()}}{
Execute benchmark
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$run()}\if{html}{\out{</div>}}
}

\subsection{Details}{
Benchmark steps:
\itemize{
\item Generate task powerset (if \code{task_powerset = TRUE})
\item Generate grid of learners and tasks
\item For each (learner, task) combo:
\itemize{
\item Tune each learner in the train set
\item Test the tuned learner's performance by taking multiple bootstrap
resamplings of the test set in the corresponding task and applying
several test measures
}
}
}

\subsection{Returns}{
a \link{tibble} with columns:
\itemize{
\item \code{task_id} => which task
\item \code{lrn_id} => which learner
\item \code{model} => the trained learner after tuning (if \code{keep_models = TRUE})
\item \code{boot_res} => a \link{BootstrapResult} object (with the bootstrap results)
}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MOBenchmark-tune_learner"></a>}}
\if{latex}{\out{\hypertarget{method-MOBenchmark-tune_learner}{}}}
\subsection{Method \code{tune_learner()}}{
Tune \code{learner} on the \code{given} task.
The tuning is done on the train set of the task (parameter \code{part}),
using a specific performance measure (parameter \code{tuning_measure_id}) and
resampling (parameter \code{tune_rsmp}).

The tuning is performed by constructing an \link[mlr3tuning:AutoTuner]{AutoTuner}
object and the optimization strategy is by default \strong{Bayesian Optimization}.
Optimization stops after a specific number of evaluations, see parameter
\code{tune_nevals}.
The hyperparameter configuration with the \strong{best} average resampled
performance is chosen to train a final model which is returned by this
function.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$tune_learner(task, learner, search_space)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{task}}{a \link[mlr3proba:TaskSurv]{TaskSurv}}

\item{\code{learner}}{a \link[mlr3proba:LearnerSurv]{LearnerSurv}}

\item{\code{search_space}}{a \link[paradox:ParamSet]{ParamSet} indicating the
learner's hyperparameters that need to be tuned.
If \code{NULL}, we just \code{train} the learner on the \code{task}.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MOBenchmark-drop_tasks"></a>}}
\if{latex}{\out{\hypertarget{method-MOBenchmark-drop_tasks}{}}}
\subsection{Method \code{drop_tasks()}}{
remove tasks from the class object to reduce size
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$drop_tasks()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MOBenchmark-drop_models"></a>}}
\if{latex}{\out{\hypertarget{method-MOBenchmark-drop_models}{}}}
\subsection{Method \code{drop_models()}}{
remove models from result tibble to reduce size
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$drop_models()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-MOBenchmark-clone"></a>}}
\if{latex}{\out{\hypertarget{method-MOBenchmark-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{MOBenchmark$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
