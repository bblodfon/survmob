% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/callbacks.R
\docType{data}
\name{xgb_es_callback}
\alias{xgb_es_callback}
\title{Early Stopping Callback for Graph XGBoost Learner}
\format{
An object of class \code{CallbackTuning} (inherits from \code{CallbackOptimization}, \code{Callback}, \code{R6}) of length 19.
}
\usage{
xgb_es_callback
}
\description{
This \link[mlr3tuning:CallbackTuning]{mlr3tuning::CallbackTuning} integrates early stopping into the
hyperparameter tuning of an XGBoost learner.
Early stopping estimates the optimal number of trees (\code{nrounds}) for a given
hyperparameter configuration.
Since early stopping is performed in each resampling iteration, there are
several optimal \code{nrounds} values.
The callback writes the maximum value to the archive in the \code{max_nrounds}
column.
In the best hyperparameter configuration (\code{instance$result_learner_param_vals}),
the value of \code{nrounds} is replaced by \code{max_nrounds} and early stopping is
deactivated.
}
\details{
This callback works only with xgboost \code{GraphLearner}s available from this
package (mainly due to using manually chosen xgboost learner ids and
hyperparameter names).
Inspired by the default early stopping callback (see \href{https://github.com/mlr-org/mlr3tuning/blob/02e9f338fad6c921ef040df8f63a8249ce6b783b/R/mlr_callbacks.R#L48-L88}{code} and
\href{https://github.com/mlr-org/mlr3tuning/issues/376}{issue}).
The callback is compatible with the \link{AutoTuner}.
The final model is fitted with the best hyperparameter configuration and
\code{max_nrounds} as \code{nrounds} i.e. early stopping is not performed.
}
\examples{
library(mlr3proba) # loads mlr3 as well
library(mlr3tuning)
library(mlr3pipelines)
library(mlr3extralearners)

task = tsk('lung')
poe = po('encode')
task = poe$train(list(task))[[1L]]

s = SurvLPS$new(nthreads = 2, ids = c('xgboost_cox_early'))
dt = s$lrn_tbl()
xgb_learner = dt$learner[[1L]]
xgb_ps = dt$param_set[[1L]]

xgb_at = AutoTuner$new(
  learner = xgb_learner,
  resampling = rsmp('holdout'),
  measure = msr('surv.rcll'),
  search_space = xgb_ps,
  terminator = trm('evals', n_evals = 5),
  tuner = tnr('random_search'),
  callbacks = xgb_es_callback
)
xgb_at$train(task)

}
\keyword{datasets}
