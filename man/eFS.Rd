% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eFS.R
\name{eFS}
\alias{eFS}
\title{Ensemble Feature Selection (eFS)}
\description{
This class stores the configuration and result of the proposed
hybrid ensemble feature selection approach, as well as the function (\code{run()})
to execute it.

We use a couple of different \strong{random survival forests} (RSFs) as learners
and a \strong{recursive feature elimination algorithm} (RFE, wrapper-based feature
selection) to find the most predictive feature subset in the given task
(dataset) for each learner.
The dataset can be subsampled before the execution of each RFE run.
This process repeats a number of times for each RSF learner and therefore
returns different best feature subsets for further analysis and processing
(e.g. finding robust features).
}
\examples{
library(mlr3proba)
library(mlr3fselect)
library(mlr3pipelines)
library(mlr3extralearners)
library(ggplot2)
set.seed(42)

# less logging
lgr::get_logger('bbotk')$set_threshold('warn')
lgr::get_logger('mlr3')$set_threshold('warn')

# Lung task - impute missing values
task = tsk('lung')
pre = po('encode', method = 'treatment') \%>>\%
      po('imputelearner', lrn('regr.rpart'))
task = pre$train(task)[[1]]

# supported lrn ids
eFS$new()$supported_lrn_ids()

# create eFS object
efs = eFS$new(lrn_ids = c('rsf_logrank', 'rsf_cindex'), nthreads_rsf = 2,
  feature_fraction = 0.6, n_features = 1, repeats = 3, subsample_ratio = 0.8
)

# useful info for RFE (feature subset sizes, mtry used)
efs$rfe_info(task)

# Execute ensemble feature selection
# In each RFE iteration, every RSF will be trained on all the features
# available and the out-of-bag error will be used to assess predictive
# performance (1 - Harrell's Cindex)
efs$run(task, verbose = TRUE)

# Get result in a tibble format
res = efs$result
res

# Get consensus performance score and number of features
median(res$score) # 1 - C-index
median(res$nfeatures)

# get frequency selection stats (per learner and consensus)
fss = efs$fs_stats()
fss$consensus # ranked consensus features

# Barplots: Feature Selection Frequency
efs$ffs_plot(lrn_id = 'consensus', title = 'RSF consensus')
efs$ffs_plot(lrn_id = 'rsf_logrank', title = 'RSF logrank')

# Performance plot (per RSF learner)
efs$res_plot(msr_label = 'OOB (1 - C-index)')

# Number of selected features plot (per RSF learner)
efs$res_plot(type = 'nfeat', ylimits = c(3,7))

# Stability plot
efs$res_plot(type = 'stab', task = task)

}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{task_id}}{(\code{character(1)})\cr
Task id}

\item{\code{lrn_ids}}{(\code{character()})\cr
Learner ids}

\item{\code{msr_id}}{(\code{character(1)})\cr
Measure id}

\item{\code{resampling}}{(\code{Resampling})\cr
\link[mlr3:Resampling]{Resampling}}

\item{\code{repeats}}{(\code{int(1)})\cr
Number of times to run RFE on each RSF learner}

\item{\code{subsample_ratio}}{(\code{double(1)})\cr
Ratio of observations to be selected in each RFE repeat}

\item{\code{n_features}}{(\code{int(1)})\cr
Number of features that signals the termination of RFE}

\item{\code{feature_fraction}}{(\code{int(1)})\cr
Fraction of features to retain in each iteration of RFE}

\item{\code{nthreads_rsf}}{(\code{int(1)})\cr
Number of cores to use in the random forest survival learners}

\item{\code{num_trees}}{(\code{int(1)})\cr
Number of trees to use in the random forest survival learners}

\item{\code{result}}{(\code{tibble})\cr
A \link{tibble} with the results from the eFS (see \code{run()} method)}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-eFS-new}{\code{eFS$new()}}
\item \href{#method-eFS-supported_lrn_ids}{\code{eFS$supported_lrn_ids()}}
\item \href{#method-eFS-rfe_info}{\code{eFS$rfe_info()}}
\item \href{#method-eFS-run}{\code{eFS$run()}}
\item \href{#method-eFS-fs_stats}{\code{eFS$fs_stats()}}
\item \href{#method-eFS-stab}{\code{eFS$stab()}}
\item \href{#method-eFS-ffs_plot}{\code{eFS$ffs_plot()}}
\item \href{#method-eFS-res_plot}{\code{eFS$res_plot()}}
\item \href{#method-eFS-help}{\code{eFS$help()}}
\item \href{#method-eFS-clone}{\code{eFS$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-new"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$new(
  lrn_ids = self$supported_lrn_ids(),
  msr_id = "oob_error",
  resampling = mlr3::rsmp("insample"),
  repeats = 100,
  subsample_ratio = 0.9,
  n_features = 2,
  feature_fraction = 0.8,
  nthreads_rsf = unname(parallelly::availableCores()),
  num_trees = 250
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{lrn_ids}}{Internal learner ids.
These will let us use the corresponding \link[mlr3:Learner]{learner}s
in the \link[mlr3fselect:AutoFSelector]{AutoFSelector}.
We get the learners using the \link{SurvLPS} class.
All ids correspond to random survival forests and by default we use all
of them.
Check available learner ids with the method \code{supported_lrn_ids()}.}

\item{\code{msr_id}}{Internal measure id.
This will let us get the \link[mlr3:Measure]{measure} to use in the
\link[mlr3fselect:AutoFSelector]{AutoFSelector}.
Must be one of the available ids provided via the \link{bench_msrs} function
or the \link[mlr3:mlr_measures_oob_error]{oob_error}.
Default is 'oob_error'.}

\item{\code{resampling}}{\link[mlr3:Resampling]{Resampling} for the
\link[mlr3fselect:AutoFSelector]{AutoFSelector}.
Default resampling is \link[mlr3:mlr_resamplings_insample]{insample}.}

\item{\code{repeats}}{Number of times to run the RFE algorithm on each RSF
learner. Defaults to 100.}

\item{\code{subsample_ratio}}{Ratio of observations to use in each RFE run.
The selection is done randomly each time and is stratified on the \code{status}
target variable (so that the censoring distribution is the same as the
original dataset).
This option enables the ensemble feature selection algorithm to be used
on different subsets of a given dataset.
Defaults to 0.9.
A value of 1 means that we always use all observations and no subsampling
is performed.}

\item{\code{n_features}}{Number of features that signals the termination of the
RFE algorithm. Defaults to 2.}

\item{\code{feature_fraction}}{Fraction of features to retain in each iteration
of the RFE algorithm. Defaults to 0.8.}

\item{\code{nthreads_rsf}}{Number of cores to use in the random forest survival
learners.
By default we use all available cores to take full advantage of
the implicit parallelization of the RSF learners.}

\item{\code{num_trees}}{Number of trees to use in the random forest survival
learners. Defaults to 250.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
\itemize{
\item By default, \code{msr_id} is \code{oob_error}:
\itemize{
\item Each RSF provides (1 - Harrell's C-index) as Out-Of-Bag error
\item An \link[mlr3:mlr_resamplings_insample]{insample resampling} is used in that
case in order to use all training data for bootstrapping in the RSFs.
}
\item \code{mtry} used in RSFs:
\itemize{
\item During the execution of the RFE algorithm, progressively smaller
feature subsets are used, as dictated by the \code{feature_fraction}
parameter.
\item For each feature subset, the RSF learner tries by default
\verb{ceiling(sqrt(#features-in-subset))} features as candidate variables
for splitting (\code{mtry}).
So \code{mtry} values also get progressively smaller.
See \code{rfe_info()} for more details.
}
}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-supported_lrn_ids"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-supported_lrn_ids}{}}}
\subsection{Method \code{supported_lrn_ids()}}{
Returns a vector of internal ids corresponding to the
random survival forest learners that can be used in method \code{run()}
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$supported_lrn_ids()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-rfe_info"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-rfe_info}{}}}
\subsection{Method \code{rfe_info()}}{
Returns the feature subset sizes that the RFE algorithm
will use and the corresponding \code{mtry} values for the
RSF learners.
These depend on the total number of features of the given task,
as well as the \code{n_features} and \code{feature_fraction}
parameters (initialized upon class construction).
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$rfe_info(task)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{task}}{\link[mlr3:Task]{mlr3::Task}}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The subset sizes are calculated from the respective
\href{https://github.com/mlr-org/mlr3fselect/blob/HEAD/R/FSelectorRFE.R}{code}
(see \code{rfe_subsets()} function, \code{ml3fselect} v0.10.0).
}

\subsection{Returns}{
a \link{tibble} with columns \code{subset_size} and \code{mtry}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-run"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-run}{}}}
\subsection{Method \code{run()}}{
Runs the ensemble feature selection on the given task.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$run(task, verbose = TRUE, store_archive = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{task}}{\link[mlr3proba:TaskSurv]{TaskSurv}}

\item{\code{verbose}}{Write log messages or not? Default: TRUE.}

\item{\code{store_archive}}{Whether to also store the
\link[mlr3fselect:ArchiveFSelect]{ArchiveFSelect}
archive object created by \code{AutoFSelector}, for debugging purposes.
Default: FALSE.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
For every RSF learner, we run \code{repeats} times the RFE algorithm
using a properly constructed \link[mlr3fselect:AutoFSelector]{AutoFSelector}).
From each \code{repeat}-ition we get the best feature subset.
The aggregated result is invisibly returned as a tibble and is available
also in the \code{result} field of this object.
}

\subsection{Returns}{
a tibble with columns:
\itemize{
\item \code{lrn_id} => which learner was used
\item \code{iter} => which iteration of RFE
\item \code{selected_features} => the best feature subset selected by RFE
\item \code{nfeatures} => how many features were selected
\item \code{score} => the performance score of the best chosen feature subset
(depends on the \code{msr_id})
\item \code{ArchiveFSelect} => additional object for validation and checking
(\strong{not included} by default due to large size)
}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-fs_stats"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-fs_stats}{}}}
\subsection{Method \code{fs_stats()}}{
Frequency selection statistics (feature ranking).
This function uses the best feature sets found by RFE
and creates one table per RSF learner with the features in descending
order of selection frequency.
A consensus frequency results table across all feature sets generated
by all RSFs during the RFE is also returned.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$fs_stats()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
A list of \code{tibble}s with columns:
\itemize{
\item \code{feat_name} => feature name
\item \code{times} => how many times a feature was chosen in the best feature
subsets across all RFE runs?
\item \code{freq} => selection frequency
}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-stab"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-stab}{}}}
\subsection{Method \code{stab()}}{
Stability assessment of the ensemble feature selection.
Currently two stability metrics are supported via the \link{stabm} R package,
namely Jaccard and Nogueira's measure.
Stability is assessed on the feature sets produced per inidividual RSF
learner used, as well as on all of them combined (consensus stability).
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$stab(task, stab_metrics = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{task}}{\link[mlr3proba:TaskSurv]{TaskSurv} that was used in \code{run()}}

\item{\code{stab_metrics}}{vector of stability metrics}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
a tibble with a \code{lrn_id} column and as many columns as the
stability measures chosen.
The \code{lrn_id} column includes the \code{consensus} stability when the feature
set from all RSFs are merged (and when more than one RSF learner was
used).
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-ffs_plot"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-ffs_plot}{}}}
\subsection{Method \code{ffs_plot()}}{
Feature selection frequency barplot
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$ffs_plot(lrn_id = "consensus", top_n = 10, title = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{lrn_id}}{One of RSF learner ids that was used during initialization
or 'consensus' (default)}

\item{\code{top_n}}{plot only the \code{n} features with the higher selection
frequency}

\item{\code{title}}{title of barplot (if NULL, the \code{lrn_id} is used)}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-res_plot"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-res_plot}{}}}
\subsection{Method \code{res_plot()}}{
Produce various plots of the eFS results
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$res_plot(
  type = "perf",
  msr_label = NULL,
  ylimits = NULL,
  title = NULL,
  include_legend = FALSE,
  task = NULL,
  stab_metrics = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{type}}{Type of plot to produce. Can be \code{perf}, \code{nfeat} or \code{stab}}

\item{\code{msr_label}}{Measure name as the y-axis label (default: \code{self$msr_id})}

\item{\code{ylimits}}{Passing on to \code{ylim()} (y-axis limits)}

\item{\code{title}}{Title for plot (default is \code{self$task_id}, the task that was
used during \code{run()})}

\item{\code{include_legend}}{By default no legend is included}

\item{\code{task}}{this is needed for \code{type} = \code{stab}. Should be the same used
when executing \code{run()}}

\item{\code{stab_metrics}}{this is needed for \code{type} = \code{stab}. By default all
stability metrics are used.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
3 types of plots can be produced:
\enumerate{
\item \strong{Performance boxplot}
\item \strong{Number of features boxplot}
\item \strong{Stability metric barplots (1 per metric)}, where also the consensus
feature set is included as a comparison category. The results are ordered
according to the values of the first metric used in \code{stab()} ('jaccard'
by default) decided by the value of \code{stab_metrics}.
}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-help"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-help}{}}}
\subsection{Method \code{help()}}{
Opens the help page for this object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$help()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-eFS-clone"></a>}}
\if{latex}{\out{\hypertarget{method-eFS-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
