% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fs.R
\name{eFS}
\alias{eFS}
\title{Ensemble Feature Selection (eFS)}
\description{
This class stores the configuration and result of the proposed
ensemble feature selection approach, as well as the function (\code{run()}) to
execute it.

We use a couple of different \strong{random survival forests} (RSFs) as learners
and a \strong{recursive feature elimination algorithm} (RFE, wrapper-based feature
selection) to find the most predictive feature subset in the given task
(dataset) for each learner.
This process repeats a number of times for each RSF learner and therefore
returns different best feature subsets for further analysis and processing
(e.g. finding robust features).
}
\examples{
library(mlr3proba)
library(mlr3fselect)
library(mlr3pipelines)
library(mlr3extralearners)

# less logging
lgr::get_logger('bbotk')$set_threshold('warn')
lgr::get_logger('mlr3')$set_threshold('warn')

# Lung task - impute missing values
task = tsk('lung')
pre = po('encode', method = 'treatment') \%>>\%
      po('imputelearner', lrn('regr.rpart'))
task = pre$train(task)[[1]]

# supported lrn ids
eFS$new()$supported_lrn_ids()

# create eFS object where every RSF is trained on each RFE feature subset
# using all features (train set equals the test set) and the out-of-bag
# error is used to assess predictive performance (1 - Cindex)
efs = eFS$new(lrn_ids = c('rsf_logrank', 'rsf_cindex'), nthreads_rsf = 4,
  feature_fraction = 0.9, n_features = 1, mtry_ratio = 0.5,
  repeats = 3, msr_id = 'oob_error', resampling = rsmp('insample')
)

# useful info for RFE (adaptive mtry.ratio, subset sizes)
efs$rfe_info(task)

# execute ensemble feature selection
efs$run(task)

# result tibble
res = efs$result
res

# get frequency selection stats (per learner and consensus)
efs$fs_stats()

}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{task_id}}{(\code{character(1)})\cr
Task id}

\item{\code{lrn_ids}}{(\code{character()})\cr
Learner ids}

\item{\code{msr_id}}{(\code{character(1)})\cr
Measure id}

\item{\code{resampling}}{(\code{Resampling})\cr
\link[mlr3:Resampling]{Resampling}}

\item{\code{repeats}}{(\code{int(1)})\cr
Number of times to run RFE on each RSF learner}

\item{\code{n_features}}{(\code{int(1)})\cr
Number of features that signals the termination of RFE}

\item{\code{feature_fraction}}{(\code{int(1)})\cr
Fraction of features to retain in each iteration of RFE}

\item{\code{nthreads_rsf}}{(\code{int(1)})\cr
Number of cores to use in the random forest survival learners}

\item{\code{num_trees}}{(\code{int(1)})\cr
Number of trees to use in the random forest survival learners}

\item{\code{mtry_ratio}}{(\code{double(1)})\cr
Percentage of features to try at each node split (should be between 0
and 1)}

\item{\code{adaptive_mr}}{(\code{logical(1)})\cr
Whether to have an adaptive \code{mtry_ratio} or not in the RFE algorithm}

\item{\code{result}}{(\code{tibble})\cr
A \link{tibble} with the results from the eFS (see \code{run()} method)}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-EnsembleFeatureSelection-new}{\code{eFS$new()}}
\item \href{#method-EnsembleFeatureSelection-supported_lrn_ids}{\code{eFS$supported_lrn_ids()}}
\item \href{#method-EnsembleFeatureSelection-rfe_info}{\code{eFS$rfe_info()}}
\item \href{#method-EnsembleFeatureSelection-run}{\code{eFS$run()}}
\item \href{#method-EnsembleFeatureSelection-fs_stats}{\code{eFS$fs_stats()}}
\item \href{#method-EnsembleFeatureSelection-clone}{\code{eFS$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EnsembleFeatureSelection-new"></a>}}
\if{latex}{\out{\hypertarget{method-EnsembleFeatureSelection-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$new(
  lrn_ids = self$supported_lrn_ids(),
  msr_id = "harrell_c",
  resampling = mlr3::rsmp("repeated_cv", repeats = 5, folds = 5),
  repeats = 100,
  n_features = 2,
  feature_fraction = 0.8,
  nthreads_rsf = parallelly::availableCores(),
  num_trees = 250,
  mtry_ratio = 0.05,
  adaptive_mr = TRUE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{lrn_ids}}{Internal learner ids.
These will let us use the corresponding \link[mlr3:Learner]{learner}s
in the \link[mlr3fselect:AutoFSelector]{AutoFSelector}.
We get the learners using the \link{SurvLPS} class.
All ids correspond to random survival forests and by default we use all
of them.
Check available learner ids with the method \code{supported_lrn_ids()}.}

\item{\code{msr_id}}{Internal measure id.
This will let us get the \link[mlr3:Measure]{measure} to use in the
\link[mlr3fselect:AutoFSelector]{AutoFSelector}.
Must be one of the available ids provided via the \link{bench_msrs} function
or the \link[mlr3:mlr_measures_oob_error]{oob_error}.
Default is Harrell's C-index.}

\item{\code{resampling}}{\link[mlr3:Resampling]{Resampling} for the
\link[mlr3fselect:AutoFSelector]{AutoFSelector}.
Default resampling is 5 times 5-fold CV.}

\item{\code{repeats}}{Number of times to run the RFE algorithm on each RSF
learner. Defaults to 100.}

\item{\code{n_features}}{Number of features that signals the termination of the
RFE algorithm. Defaults to 2.}

\item{\code{feature_fraction}}{Fraction of features to retain in each iteration
of the RFE algorithm. Defaults to 0.8.}

\item{\code{nthreads_rsf}}{Number of cores to use in the random forest survival
learners.
By default we use all available cores to take full advantage of
the implicit parallelization of the RSF learners.}

\item{\code{num_trees}}{Number of trees to use in the random forest survival
learners. Defaults to 250.}

\item{\code{mtry_ratio}}{Percentage of features to try at each tree node split
(should be between 0 and 1).
Default value is 0.05 which means that 500 features will be randomly
selected in a node split, considering a dataset of 10000 features.}

\item{\code{adaptive_mr}}{Whether to use an adaptive \code{mtry_ratio} or not.
Default: TRUE.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
\itemize{
\item If \code{msr_id} is \code{oob_error}:
\itemize{
\item Make sure to check which is the \code{oob_error} provided by each RSF
learner (should be the same ideally, usually 1 - Cindex)
\item It's more efficient to have an \link[=mlr_resamplings_insample]{insample resampling}
in that case.
}
\item \code{adaptive_mr}:
\itemize{
\item During the execution of the RFE algorithm, progressively smaller
feature subsets are used, as dictated by the \code{feature_fraction}
parameter.
\item Setting \code{adaptive_mr} to FALSE means that the same value of \code{mtry_ratio}
is used, irrespective of the number of features in each subset.
This creates the following issue: when the feature subsets get really
smaller (e.g. less 40 features) and with an example \code{mtry_ratio} of
0.05, we get only 2 features per tree node split, which results in
extremely randomized trees.
\item Setting \code{adaptive_mr} to TRUE (default), the value of \code{mtry_ratio}
changes in each iteration of the RFE algorithm according to the
following formula:
\deqn{mr^{log(s)/log(n)}}, where \code{mr} is the \code{mtry_ratio} provided
at initialization, \code{subset_size} is the current number of
features in the RFE subset and \code{n} is the total number of features
in the original dataset.
The formula results in an increase of the \code{mtry_ratio}s in the RSFs
as the feature subsets selected by the RFE algorithm get smaller.
Therefore more bagged trees are being created with smaller feature
subsets.
}
}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EnsembleFeatureSelection-supported_lrn_ids"></a>}}
\if{latex}{\out{\hypertarget{method-EnsembleFeatureSelection-supported_lrn_ids}{}}}
\subsection{Method \code{supported_lrn_ids()}}{
Returns a vector of internal ids corresponding to the
random survival forest learners that can be used in method \code{run()}
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$supported_lrn_ids()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EnsembleFeatureSelection-rfe_info"></a>}}
\if{latex}{\out{\hypertarget{method-EnsembleFeatureSelection-rfe_info}{}}}
\subsection{Method \code{rfe_info()}}{
Returns the feature subset sizes that the RFE algorithm
will use and the corresponding \code{mtry_ratio} and \code{mtry} values for the
RSF learners.
These depend on the total number of features of the given task,
as well as the \code{n_features} and \code{feature_fraction} parameters
(initialized upon class construction).
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$rfe_info(task)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{task}}{\link[mlr3:Task]{mlr3::Task}}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The subset sizes are calculated from the respective
\href{https://github.com/mlr-org/mlr3fselect/blob/HEAD/R/FSelectorRFE.R#L123}{code}.
}

\subsection{Returns}{
a \link{tibble} with columns \code{subset_size}, \code{mtry_ratio} and \code{mtry}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EnsembleFeatureSelection-run"></a>}}
\if{latex}{\out{\hypertarget{method-EnsembleFeatureSelection-run}{}}}
\subsection{Method \code{run()}}{
Runs the ensemble feature selection on the given task.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$run(task, verbose = TRUE, store_archive = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{task}}{\link[mlr3proba:TaskSurv]{TaskSurv}}

\item{\code{verbose}}{Write log messages or not? Default: TRUE.}

\item{\code{store_archive}}{Whether to also store the \link[mlr3fselect:ArchiveFSelect]{ArchiveFSelect}
archive object created by \code{AutoFSelector}, for debugging purposes.
Default: FALSE.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
For every RSF learner, we run \code{repeats} times the RFE algorithm
using a properly constructed \link[mlr3fselect:AutoFSelector]{AutoFSelector}).
From each \code{repeat}ition we get the best feature subset.
The aggregated result is invisibly returned as a tibble and is available
also in the \code{result} field of this object.
}

\subsection{Returns}{
a tibble with columns:
\itemize{
\item \code{lrn_id} => which learner was used
\item \code{iter} => in this particular execution of RFE (out of a total \code{repeats})
\item \code{selected_features} => the best feature subset selected by RFE
\item \code{nfeatures} => how many features were selected
\item \code{score} => the performance score of the chosen best feature subset
(depends on the \code{msr_id})
\item \code{ArchiveFSelect} => additional object for validation and checking
(\strong{not included} by default due to large size)
}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EnsembleFeatureSelection-fs_stats"></a>}}
\if{latex}{\out{\hypertarget{method-EnsembleFeatureSelection-fs_stats}{}}}
\subsection{Method \code{fs_stats()}}{
Frequency selection statistics.
This function uses the best feature subsets found by RFE
and creates one table per RSF learner with the features in descending
order of selection frequency.
A consensus frequency results table across all feature subsets generated
by all RSFs in the RFE is also returned.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$fs_stats()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
A list of \code{tibble}s with columns:
\itemize{
\item \code{feat_name} => feature name
\item \code{times} => how many times a feature was chosen in the best feature
subsets across all RFE runs?
\item \code{freq} => selection frequency
}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-EnsembleFeatureSelection-clone"></a>}}
\if{latex}{\out{\hypertarget{method-EnsembleFeatureSelection-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{eFS$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
